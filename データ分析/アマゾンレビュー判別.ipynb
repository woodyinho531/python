{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Amazonに投稿された映画のレビュー(英語)を分析し、レビューがPositiveかNegativeかの判別を行う\n",
    "\n",
    "Training_data (positive用)、文章数 : 700\n",
    "Training_data (negative用)、文章数 : 700\n",
    "Test_data (positive用)、文章数 : 3\n",
    "Test_data (negative用)、文章数 : 3\n",
    "（ ※学習用データ：1400、　テスト用データ：6、　合計 : 1406 の文章です\n",
    "\n",
    "'''\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     neg: loaded 700 reviews.\n",
      "     pos: loaded 700 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Training_data の読み込み\n",
    "datapath = Path('Training_data')\n",
    "review_pattern = re.compile(r'cv\\d+')  # ファイル名が \"cv数字\"　で始まるファイル名かを調べる正規表現\n",
    "test_size = 0.25\n",
    "\n",
    "data_orig  = dict(neg=[], pos=[])\n",
    "data_train = dict(neg=[], pos=[])    # 学習データ\n",
    "data_verify  = dict(neg=[], pos=[])    # 検証データ\n",
    "\n",
    "# data_origへのデータの読み込み　と　train/verifyへのデータの分割\n",
    "for cls, review in data_orig.items():\n",
    "    for path in (datapath / cls).iterdir():\n",
    "        if review_pattern.match(path.name):\n",
    "            with open(path, 'r', encoding='latin') as src:\n",
    "                review.append(src.read())\n",
    "    print(f\"{cls:>8}: loaded {len(review)} reviews.\")\n",
    "\n",
    "    data_train[cls], data_verify[cls] = train_test_split(review, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     neg: loaded 3 reviews.\n",
      "     pos: loaded 3 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Test_data の読み込み\n",
    "datapath_test = Path('Test_data')\n",
    "review_test_pattern = re.compile(r'amazon_review_\\d+')  # ファイル名が \"amazon_review_数字\"　で始まるファイル名かを調べる正規表現\n",
    "data_test  = dict(neg=[], pos=[])    # テストデータ\n",
    "\n",
    "# data_testへのデータの読み込み\n",
    "for cls, review in data_test.items():\n",
    "    for path in (datapath_test / cls).iterdir():\n",
    "        if review_test_pattern.match(path.name):\n",
    "            with open(path, 'r', encoding='latin') as src:\n",
    "                review.append(src.read())\n",
    "    print(f\"{cls:>8}: loaded {len(review)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの整形用の関数　get_values_and_targets　を定義する\n",
    "def get_values_and_targets(data):\n",
    "    values = data['neg'] + data['pos']\n",
    "    target = [True]*len(data['neg']) + [False]*len(data['pos'])\n",
    "    target = np.array(target)\n",
    "    return values, target\n",
    "\n",
    "# data_trainに対して get_values_and_targets を実行する\n",
    "values_train, target_train = get_values_and_targets(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bad: Disregard for plot points from the lead up movies, including the first Avengers (mentioned, but stripped of all layers). Characters having sudden new and unexplained personalities (bound to happen when they're juggling so damned many!). CGI is a wonderful tool, but when you see Spider-Man's head floating above his CGI body (no he did not get decapitated), it pulls you out of the narrative. The focus of the film was on too many characters who were not relevant to the central plot, maybe the excuse was to have an epic Final Battle scene in Wakanda (SPOILER: Some of those dead characters are needed for a Black Panther sequel, further proving my point).\n",
      "\n",
      "The Ugly: One character death after another, with so many dying that they often skip to the aftermath or just state that Thanos killed them, it really pulls at the heartstrings... Except, as this review title implies, the movie has zero lasting effects. They wasted no time in throwing out all credibility (SPOILER: Thor caused the extinction of the Asgardians), so the rest is just a fun What If? movie focused on committing the Woman In Fridge clichÃ© as many characters as possible.\n"
     ]
    }
   ],
   "source": [
    "# 読み込んだファイルの中身を表示（テストデータ, Negative, 1つ目）\n",
    "print(data_test['neg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x274 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 274 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの前処理を指定して、特徴量ベクトルを作成\n",
    "def nullify_symbols(text):\n",
    "    for ch in \".,:;!?-+*/=()[]{}<>~^#$@%&'\\\"_0123456789\":\n",
    "        text = text.replace(ch, ' ')\n",
    "    return text\n",
    "def format_words(words, min_length=3):\n",
    "    return [word.lower() for word in words if len(word) >= min_length]\n",
    "format_words(nullify_symbols(data_train['neg'][0]).split())\n",
    "vocab = CountVectorizer(token_pattern=r'[a-zA-Z]{3,}').fit([data_train['neg'][0]])\n",
    "vocab.transform([data_train['neg'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t3\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t2\n",
      "  (0, 10)\t2\n",
      "  (0, 11)\t14\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t7\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t2\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t1\n",
      "  :\t:\n",
      "  (0, 249)\t1\n",
      "  (0, 250)\t1\n",
      "  (0, 251)\t1\n",
      "  (0, 252)\t1\n",
      "  (0, 253)\t1\n",
      "  (0, 254)\t1\n",
      "  (0, 255)\t1\n",
      "  (0, 256)\t1\n",
      "  (0, 257)\t1\n",
      "  (0, 258)\t1\n",
      "  (0, 259)\t4\n",
      "  (0, 260)\t1\n",
      "  (0, 261)\t1\n",
      "  (0, 262)\t3\n",
      "  (0, 263)\t1\n",
      "  (0, 264)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 266)\t4\n",
      "  (0, 267)\t1\n",
      "  (0, 268)\t1\n",
      "  (0, 269)\t1\n",
      "  (0, 270)\t1\n",
      "  (0, 271)\t1\n",
      "  (0, 272)\t1\n",
      "  (0, 273)\t5\n"
     ]
    }
   ],
   "source": [
    "# print() で特徴量データを表示\n",
    "print(vocab.transform([data_train['neg'][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特徴量を配列形式で表示\n",
    "vocab    = CountVectorizer(binary=True, token_pattern=r'[a-zA-Z]{3,}')\n",
    "features = vocab.fit_transform(values_train)\n",
    "features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 30982)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特徴量のshapeを確認\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティックモデルを採用してフィッティングを行う\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter=1000, random_state=539167).fit(features, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_verifyに対して get_values_and_targets を実行する\n",
    "values_verify, target_verify = get_values_and_targets(data_verify)\n",
    "\n",
    "# モデルに分類させる\n",
    "features_verify = vocab.transform(values_verify)\n",
    "pred_verify     = model.predict(vocab.transform(values_verify))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/350 correct (80.857%)\n"
     ]
    }
   ],
   "source": [
    "# 正答率を確認\n",
    "validation = (pred_verify  == target_verify )\n",
    "size    = validation.size\n",
    "correct = np.count_nonzero(validation)\n",
    "print(f\"{correct}/{size} correct ({correct*100/size:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     neg:   3/  3 correct (100.000%)\n",
      "     pos:   3/  3 correct (100.000%)\n"
     ]
    }
   ],
   "source": [
    "# data_testのカテゴリごとに正答率を確認\n",
    "for cls in ('neg', 'pos'):\n",
    "    _values  = data_test[cls]\n",
    "    _is_spam = [(cls == 'neg')]*len(_values)\n",
    "    _pred    = model.predict(vocab.transform(_values))\n",
    "    _valid   = (_pred == _is_spam)\n",
    "    _size    = _valid.size\n",
    "    _correct = np.count_nonzero(_valid)\n",
    "    print(f\"{cls:>8s}: {_correct:>3d}/{_size:>3d} correct ({_correct*100/_size:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
